#### A/B 测试（A/B testing）
用于比较产品 A 和产品 B 的收益。A/B 测试包含两个阶段：首先是探索阶段，即以相同的比例测试两款产品，从中找出表现更好的产品；然后是利用阶段，即向更好的产品投入所有资源，以期实现利润最大化。进行 A/B 测试的关键是在探索阶段和利用阶段之间取得平衡。

#### epsilon 递减策略（epsilon-decreasing strategy）
这种强化学习技术用于分配资源，它包括两个彼此交叉的阶段：探索阶段和利用阶段。epsilon 指探索时间与总时间的比例，随着最佳方案的相关信息越来越多，epsilon 值逐渐减小。

#### k 均值聚类（k-means clustering）
这种无监督学习技术用于把相似的数据点划入同一个群组，其中 k 指群组数量。

#### k 最近邻（k-Nearest Neighbors）
这种监督学习技术根据某个数据点周围距离最近的数据点的类型对该数据点进行分类，其中 k 是用作参考的数据点的个数。

#### Louvain 方法（Louvain method）
这种无监督学习方法用于找出网络中的群组，其采用的方式是将群组内部的相互作用最大化，同时把群组之间的相互作用最小化。

#### PageRank 算法（PageRank algorithm）
用于找出网络中占主导地位的节点。它基于节点的链接数以及链接的强度和来源对节点进行排序。

#### 变量（variable）
用于描述数据点。变量又叫属性、特征或维度，包括如下几类。

##### 二值变量（binary variable）
最简单的变量类型，它只有两个可选值（比如性别）。

##### 分类变量（categorical variable）
这种变量可以用来表示有两个以上选择的情况（比如种族）。

##### 整型变量（integer variable）
这种变量用来表示整数（比如年龄）。

##### 连续变量（continuous variable）
这种变量最为精细，用来表示小数（比如价格）。

#### 标准化（standardization）
用于把所有变量统一到一个标准尺度上，类似于使用百分位数表示每个变量。

#### 参数调优（parameter tuning）
这是一个调整算法设置的过程，目标是提高模型的预测准确度，就像调节收音机的频道一样。

#### 测试集（test dataset）
用于评估预测模型的准确度和泛化能力。先用训练集生成模型，而后用测试集来测试模型。

#### 递归拆分（recursive partitioning）
指反复拆分数据样本以得到同质组。决策树的生成过程就涉及递归拆分。

#### 丢弃（dropout）
用于防止神经网络模型出现过拟合问题。每次训练期间，随机丢弃一些神经元，以此迫使不同的神经元协同工作，以揭示训练样本的更多特征。

#### 陡坡图（scree plot）
用于确定合适的群组数量。陡坡图有着广泛的应用，从聚类到降维都能看到它的身影。最佳群组数量通常出现在陡坡图曲线的拐弯处。如果允许有更多的群组，可能会导致模型的泛化能力下降。

#### 多臂老虎机问题（multi-arm bandit problem）
指资源分配问题，比如选择哪台老虎机下注。多臂老虎机这个名字源于老虎机的绰号“独臂强盗”。之所以有这样一个绰号，是因为老虎机似乎仅凭一条手臂（拉杆）就能骗走玩家的钱。

#### 多重共线性（multicollinearity）
这是回归分析中的一个问题。如果回归模型包含高度相关的预测变量，那么这些变量的权重会失真。

#### 二次取样（subsampling）
用于防止神经网络模型出现过拟合问题，具体做法是通过取平均值对输入的训练数据进行“平滑化”处理。比如，可以通过二次取样缩小图像尺寸或降低颜色对比度。

#### 反向传播（backpropagation）
指在神经网络中给出有关预测是否准确的反馈。预测错误会沿着路径反向传播，这条路径上的神经元会重新调整其激活条件，以减少错误。

#### 分类（classiﬁcation）
这是对一类监督学习技术的统称，运用这些技术，可以预测二元值和分类值。

#### 关联规则（association rule）
这是一个无监督学习技术，用来揭示数据点之间是如何关联的，比如找出顾客经常同时购买哪些商品。识别关联规则的常用指标有 3 个：

##### {X} 的支持度
表示 X 项出现的频率；

##### {X → Y} 的置信度
表示当 X 项出现时 Y 项同时出现的频率；

##### {X → Y} 的提升度
表示 X 项和 Y 项一同出现的频率，并且考虑每项各自出现的频率。

#### 过拟合（overﬁtting）
发生过拟合时，预测模型对数据中的随机波动过于敏感，并且将其误以为是持久模式。过拟合模型对当前数据有很高的预测准确度，但是泛化能力不强，即对未知数据的预测效果不佳。

#### 核技巧（kernel trick）
用于把数据点映射到高维空间。在高维空间中，可以使用直线把数据点分开。这些直线容易计算，并且当映射回低维空间时也很容易转换成曲线。

#### 黑盒（black box）
这个术语用来描述不可解释的预测模型。在这样的模型中，不存在可用于推导预测结果的明确公式。

#### 回归分析（regression analysis）
这种监督学习技术用于找出最佳拟合线，使得尽可能多的数据点位于这条线附近。最佳拟合线由带权重的组合预测变量得到。

#### 混淆矩阵（confusion matrix）
用于评价分类预测模型的准确度。除了总体分类准确度之外，混淆矩阵还会给出假正例率和假负例率。

#### 集成方法（ensembling）
用于组合多个预测模型，借以提高预测准确度。集成方法之所以非常有效，是因为正确的预测结果往往彼此强化，错误的预测结果则相互抵消。

#### 激活规则（activation rule）
用于指定激活神经元所必需的输入信号的来源和强度。神经元的激活状态在神经网络中传播，最后产生预测结果。

#### 监督学习（supervised learning）
这是对一类机器学习算法的统称。之所以把这些算法称为监督学习算法，是因为它们的预测都基于数据中已有的模式。

#### 降维（dimension reduction）
指减少变量的个数，比如通过组合高度相关的变量来实现。

#### 交叉验证（cross-validation）
这个方法通过把数据集划分成若干组来对模型进行反复测试，从而最大限度地利用可用的数据。在单次迭代中，除了某一组之外，其他各组都被用来训练预测模型，而后使用留下的那组测试模型。这个过程会重复进行，直到每一组都测试过模型，并且只测试过一次。模型的最终预测准确度取所有迭代评估结果的平均值。

#### 决策树（decision tree）
这种监督学习技术通过一系列二元选择题来拆分数据样本，以获得同质组。虽然决策树容易理解和可视化，但也容易出现过拟合问题。

#### 均方根误差（root mean squared error）
这个指标用来评价回归预测的准确度，尤其可用于避免较大的误差。因为每个误差都要取平方，所以大误差会被放大，这使得该指标对异常值极其敏感。

#### 平移不变性（translational invariance）
这是卷积神经网络的一个特性，指的是图像特征的位置并不影响神经网络对这些特征的识别。

#### 欠拟合（underﬁtting）
发生欠拟合时，预测模型过于迟钝，以至于忽略了数据中的基本模式。欠拟合模型很可能忽视数据中的重要趋势，这会导致预测模型对当前数据和未知数据的预测准确度较差。

#### 强化学习（reinforcement learning）
这是对一类机器学习算法的统称，指使用数据中的模式做预测，并根据越来越多的反馈结果不断改进。

#### 神经网络（neural network）
这种监督学习技术使用神经元层来进行学习和预测。虽然神经网络的预测准确度很高，但其复杂性使得大部分预测结果难以解释。

#### 随机森林（random forest）
这种监督学习技术通过随机选择不同的二元选择题来生成多棵决策树，然后综合这些决策树的预测结果。

#### 特征工程（feature engineering）
指创造性地产生新变量的过程，比如通过重新编码生成一个变量，或组合多个变量。

#### 梯度提升（gradient boosting）
这种监督学习技术用于生成多棵决策树。与随机森林不同，梯度提升通过有策略地选择不同的二元选择题来生成每个分支，从而逐步提高决策树的预测准确度。然后，为每棵树的预测结果赋予一定的权重（决策树越靠后，权重越大），并组合所有结果，从而产生最终的预测结果。

#### 梯度下降（gradient descent）
这种方法用于调整模型参数。它先为一组参数值估计初始值，而后通过一个迭代过程，把这些估计值应用于每个数据点做预测，然后调整估计值，以减少整体预测误差。

#### 无监督学习（unsupervised learning）
这是对一类机器学习算法的统称，这些算法用于发现数据中的隐藏模式。之所以把这些算法称为无监督学习算法，是因为我们并不知道要找的模式是什么，而是要依靠算法来发现。

#### 先验原则（apriori principle）
如果某个项集出现得不频繁，那么包含它的任何更大的项集必定也出现得不频繁。先验原则有助于减少需要考虑的项集组合的个数。

#### 相关系数（correlation coefﬁcient）
用于衡量两个变量之间的线性关系。相关系数的取值范围是 –1 到 1，它提供了两部分信息。

##### 关联强度
当相关系数为–1 或1时，关系最强；当相关系数为0时，关系最弱。

##### 关联方向
当两个变量同向变化时，相关系数为正，否则为负。

#### 训练集（training dataset）
用于生成预测模型。模型生成之后，再用测试集评估模型的预测准确度。

#### 验证（validation）
指评估模型对新数据的预测准确度。具体做法是把当前的数据集划分成两部分：一部分是训练集，用来生成和调整预测模型；另一部分是测试集，用来充当新数据并评估模型的预测准确度。

#### 正则化（regularization）
用于防止预测模型出现过拟合问题，具体做法是引入惩罚参数，通过人为增大预测误差对模型复杂度的增加进行惩罚。这使得我们在优化模型参数时需要同时考虑复杂度和准确度。

#### 支持向量机（support vector machine）
这种监督学习技术用于把数据点分为两组，具体做法是在两组的外围数据点（也叫支持向量）的中间画一条分界线。它使用核技巧来高效地求得带凸弧的决策边界。

#### 主成分分析（principal component analysis）
这种无监督学习技术把数据中富含信息的变量组合成新变量，以此减少要分析的变量个数。新变量被称为主成分。

#### 自助聚集法（bootstrap aggregating）
用于生成数千棵彼此不相关的决策树，它们共同产生预测结果，从而避免出现过拟合问题。每棵树由训练数据的一个随机子集生成，并且每次拆分时都选用预测变量的一个随机子集。

#### 最佳拟合线（best-ﬁt line）
回归分析常用的趋势线，它使绝大部分数据点都位于其附近。